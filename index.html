<!DOCTYPE html>


<html lang="en" data-content_root="./">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <title>Interpretability Techniques for Speech Models &#8212; Tutorial @ Interspeech 2025</title>



  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-J3L8X6LZR0"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-J3L8X6LZR0');
  </script>

  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />


  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

  <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
  <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
  <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
  <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
  <link rel="stylesheet" type="text/css"
    href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
  <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
  <link rel="stylesheet" type="text/css"
    href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  <link rel="stylesheet" type="text/css" href="_static/custom.css?v=3821e700" />

  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

  <script src="_static/documentation_options.js?v=9eb32ce0"></script>
  <script src="_static/doctools.js?v=888ff710"></script>
  <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
  <script src="_static/clipboard.min.js?v=a7894cd8"></script>
  <script src="_static/copybutton.js?v=f281be69"></script>
  <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
  <script>let toggleHintShow = 'Click to show';</script>
  <script>let toggleHintHide = 'Click to hide';</script>
  <script>let toggleOpenOnPrint = 'true';</script>
  <script src="_static/togglebutton.js?v=4a39c7ea"></script>
  <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
  <script src="_static/design-tabs.js?v=36754332"></script>
  <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
  <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
  <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
  <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
  <script>DOCUMENTATION_OPTIONS.pagename = 'index';</script>
  <link rel="icon" href="_static/favicon.ico" />
  <link rel="index" title="Index" href="genindex.html" />
  <link rel="search" title="Search" href="search.html" />
  <link rel="next" title="TBA, check back soon!" href="interspeech2025/intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="docsearch:language" content="en" />
</head>


<body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%"
  data-default-mode="">



  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>

  <div id="pst-scroll-pixel-helper"></div>

  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>


  <input type="checkbox" class="sidebar-toggle" name="__primary" id="__primary" />
  <label class="overlay overlay-primary" for="__primary"></label>

  <input type="checkbox" class="sidebar-toggle" name="__secondary" id="__secondary" />
  <label class="overlay overlay-secondary" for="__secondary"></label>

  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
      <form class="bd-search d-flex align-items-center" action="search.html" method="get">
        <i class="fa-solid fa-magnifying-glass"></i>
        <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..."
          aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off"
          spellcheck="false" />
        <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
      </form>
    </div>
  </div>

  <header class="bd-header navbar navbar-expand-lg bd-navbar">
  </header>


  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">





      <div class="bd-sidebar-primary bd-sidebar">



        <div class="sidebar-header-items sidebar-primary__section">




        </div>

        <div class="sidebar-primary-items__start sidebar-primary__section">
          <div class="sidebar-primary-item">



            <a class="navbar-brand logo" href="#">










              <img src="_static/logo.png" class="logo__image only-light" alt="Tutorial @ Interspeech 2025 - Home" />
              <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Tutorial @ Interspeech 2025 - Home"/>`);</script>


            </a>
          </div>
          <div class="sidebar-primary-item">

            <script>
              document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
            </script>
          </div>
          <div class="sidebar-primary-item">
            <nav class="bd-links bd-docs-nav" aria-label="Main">
              <div class="bd-toc-item navbar-nav active">

                <ul class="nav bd-sidenav bd-sidenav__home-link">
                  <li class="toctree-l1 current active">
                    <a class="reference internal" href="#">
                      Interpretability Techniques for Speech Models
                    </a>
                  </li>
                </ul>
                <p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorial Materials</span>
                </p>
                <ul class="nav bd-sidenav">
                  <li class="toctree-l1"><a class="reference internal" href="interspeech2025/intro.html">TBA, check back
                      soon!</a></li>
                </ul>

              </div>
            </nav>
          </div>
        </div>


        <div class="sidebar-primary-items__end sidebar-primary__section">
        </div>

        <div id="rtd-footer-container"></div>


      </div>

      <main id="main-content" class="bd-main">



        <div class="sbt-scroll-pixel-helper"></div>

        <div class="bd-content">
          <div class="bd-article-container">

            <div class="bd-header-article">
              <div class="header-article-items header-article__inner">

                <div class="header-article-items__start">

                  <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm"
                      for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom"
                      data-bs-toggle="tooltip">
                      <span class="fa-solid fa-bars"></span>
                    </label></div>

                </div>


                <div class="header-article-items__end">

                  <div class="header-article-item">

                    <div class="article-header-buttons">





                      <div class="dropdown dropdown-download-buttons">
                        <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown"
                          aria-expanded="false" aria-label="Download this page">
                          <i class="fas fa-download"></i>
                        </button>
                        <ul class="dropdown-menu">



                          <li><a href="_sources/index.md" target="_blank"
                              class="btn btn-sm btn-download-source-button dropdown-item" title="Download source file"
                              data-bs-placement="left" data-bs-toggle="tooltip">


                              <span class="btn__icon-container">
                                <i class="fas fa-file"></i>
                              </span>
                              <span class="btn__text-container">.md</span>
                            </a>
                          </li>




                          <li>
                            <button onclick="window.print()" class="btn btn-sm btn-download-pdf-button dropdown-item"
                              title="Print to PDF" data-bs-placement="left" data-bs-toggle="tooltip">


                              <span class="btn__icon-container">
                                <i class="fas fa-file-pdf"></i>
                              </span>
                              <span class="btn__text-container">.pdf</span>
                            </button>
                          </li>

                        </ul>
                      </div>




                      <button onclick="toggleFullScreen()" class="btn btn-sm btn-fullscreen-button"
                        title="Fullscreen mode" data-bs-placement="bottom" data-bs-toggle="tooltip">


                        <span class="btn__icon-container">
                          <i class="fas fa-expand"></i>
                        </span>

                      </button>



                      <script>
                        document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
                      </script>


                      <script>
                        document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
                      </script>
                      <label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"
                        title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
                        <span class="fa-solid fa-list"></span>
                      </label>
                    </div>
                  </div>

                </div>

              </div>
            </div>



            <div id="jb-print-docs-body" class="onlyprint">
              <h1>Interpretability Techniques for Speech Models</h1>
              <!-- Table of contents -->
              <div id="print-main-content">
                <div id="jb-print-toc">

                  <div>
                    <h2> Contents </h2>
                  </div>
                  <nav aria-label="Page">
                    <ul class="visible nav section-nav flex-column">
                      <li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link"
                          href="#preliminary-programme-at-interspeech-2025">Preliminary programme at Interspeech
                          2025</a></li>
                      <li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link"
                          href="#references">References</a></li>
                    </ul>
                  </nav>
                </div>
              </div>
            </div>



            <div id="searchbox"></div>
            <article class="bd-article">

              <head>
                <meta property="og:image"
                  content="https://raw.githubusercontent.com/interpretingdl/speech-interpretability-tutorial/refs/heads/main/book/images/tutorial-overview.png">
              </head>
              <section class="tex2jax_ignore mathjax_ignore" id="interpretability-techniques-for-speech-models">
                <h1>Interpretability Techniques for Speech Models<a class="headerlink"
                    href="#interpretability-techniques-for-speech-models" title="Link to this heading">#</a></h1>
                <p>Pre-trained foundation models have revolutionized speech technology like many other adjacent fields.
                  The combination of their capability and opacity has sparked interest in researchers trying to
                  interpret the models in various ways. While interpretability in fields such as computer vision and
                  natural language processing has made significant progress towards understanding model internals and
                  explaining their decisions, speech technology has lagged behind despite the widespread use of complex,
                  black-box neural models. Recent studies have begun to address this gap, marked by a growing body of
                  literature focused on interpretability in the speech domain. This tutorial provides a structured
                  overview of interpretability techniques, their applications, implications, and limitations when
                  applied to speech models, aiming to help researchers and practitioners better understand, evaluate,
                  debug, and optimize speech models while building trust in their predictions. In hands-on sessions,
                  participants will explore how speech models encode distinct features (e.g., linguistic information)
                  and utilize them in their inference. By the end, attendees will be equipped with the tools and
                  knowledge to start analyzing and interpreting speech models in their own research, potentially
                  inspiring new directions.</p>
                <div class="admonition note">
                  <p class="admonition-title">Note</p>
                  <p>We will present our tutorial about <em>Interpretability Techniques for Speech Models</em> on
                    <strong>Sunday, August 17th</strong> at this year’s Interspeech conference in Rotterdam. <br> Check
                    out the <a class="reference internal" href="#interspeech-programme">preliminary programme</a> below,
                    and sign up through the <a class="reference external"
                      href="https://www.interspeech2025.org/registration">Interspeech registration form</a>!
                  </p>
                </div>
                <p><img alt="tutorial-overview-diagram" src="_images/tutorial-overview.png" /></p>
                <section id="preliminary-programme-at-interspeech-2025">
                  <span id="interspeech-programme"></span>
                  <h2>Preliminary programme at Interspeech 2025<a class="headerlink"
                      href="#preliminary-programme-at-interspeech-2025" title="Link to this heading">#</a></h2>
                  <blockquote>
                    <div>
                      <p><strong>Introduction</strong> to challenges of speech data for interpretability research <br>
                        <a class="reference external" href="https://grzegorz.chrupala.me/">Grzegorz Chrupała</a>
                      </p>
                    </div>
                  </blockquote>
                  <blockquote>
                    <div>
                      <p>Tutorial on <strong>Representational Analysis methods</strong> for speech model
                        interpretability, including: <br> Probing<span id="id1"><sup><a class="reference internal"
                              href="#id13"
                              title="Martijn Bentum, Louis ten Bosch, and Tom Lentz. The Processing of Stress in End-to-End Automatic Speech Recognition Models. In Interspeech 2024, 2350–2354. 2024. doi:10.21437/Interspeech.2024-44.">1</a>,<a
                              class="reference internal" href="#id14"
                              title="Gaofei Shen, Michaela Watkins, Afra Alishahi, Arianna Bisazza, and Grzegorz Chrupała. Encoding of lexical tone in self-supervised models of spoken language. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 4250–4261. Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi:10.18653/v1/2024.naacl-long.239.">2</a>,<a
                              class="reference internal" href="#id15"
                              title="Marianne de Heer Kloots, Hosein Mohebbi, Charlotte Pouw, Gaofei Shen, Willem Zuidema, and Martijn Bentum. What do self-supervised speech models know about Dutch? Analyzing advantages of language-specific pre-training. In Interspeech 2025. 2025. doi:10.48550/arXiv.2506.00981.">3</a>,<a
                              class="reference internal" href="#id16"
                              title="Patrick Cormac English, John D. Kelleher, and Julie Carson-Berndsen. Domain-Informed Probing of wav2vec 2.0 Embeddings for Phonetic Features. In Garrett Nicolai and Eleanor Chodroff, editors, Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, 83–91. Seattle, Washington, July 2022. Association for Computational Linguistics. doi:10.18653/v1/2022.sigmorphon-1.9.">4</a></sup></span>,
                        Representational Similarity Analyses (RSA<span id="id2"><sup><a class="reference internal"
                              href="#id18"
                              title="Grzegorz Chrupała, Bertrand Higy, and Afra Alishahi. Analyzing analytical methods: the case of phonology in neural models of spoken language. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 4146–4156. Online, July 2020. Association for Computational Linguistics. doi:10.18653/v1/2020.acl-main.381.">5</a>,<a
                              class="reference internal" href="#id17"
                              title="Gaofei Shen, Afra Alishahi, Arianna Bisazza, and Grzegorz Chrupała. Wave to Syntax: Probing spoken language models for syntax. In Interspeech 2023, 1259–1263. 2023. doi:10.21437/Interspeech.2023-679.">6</a></sup></span>,
                        CCA<span id="id3"><sup><a class="reference internal" href="#id19"
                              title="Ankita Pasad, Ju-Chieh Chou, and Karen Livescu. Layer-wise analysis of a self-supervised speech representation model. In 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), volume, 914-921. 2021. doi:10.1109/ASRU51503.2021.9688093.">7</a></sup></span>,
                        CKA<span id="id4"><sup><a class="reference internal" href="#id20"
                              title="Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, 3519–3529. PMLR, 09–15 Jun 2019. URL: https://proceedings.mlr.press/v97/kornblith19a.html.">8</a></sup></span>),
                        <br> CTC &amp; Decoder lenses<span id="id5"><sup><a class="reference internal" href="#id21"
                              title="Marianne de Heer Kloots and Willem Zuidema. Human-like Linguistic Biases in Neural Speech Models: Phonetic Categorization and Phonotactic Constraints in Wav2Vec2.0. In Interspeech 2024, 4593–4597. 2024. doi:10.21437/Interspeech.2024-2490.">9</a>,<a
                              class="reference internal" href="#id22"
                              title="Anna Langedijk, Hosein Mohebbi, Gabriele Sarti, Willem Zuidema, and Jaap Jumelet. DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Findings of the Association for Computational Linguistics: NAACL 2024, 4764–4780. Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi:10.18653/v1/2024.findings-naacl.296.">10</a></sup></span>,
                        ABX tests<span id="id6"><sup><a class="reference internal" href="#id23"
                              title="Thomas Schatz. ABX-Discriminability Measures and Applications. Theses, Université Paris 6 (UPMC), September 2016. URL: https://hal.science/tel-01407461.">11</a>,<a
                              class="reference internal" href="#id24"
                              title="Robin Algayres, Tristan Ricoul, Julien Karadayi, Hugo Laurençon, Salah Zaiem, Abdelrahman Mohamed, Benoît Sagot, and Emmanuel Dupoux. DP-Parse: Finding Word Boundaries from Raw Speech with an Instance Lexicon. Transactions of the Association for Computational Linguistics, 10:1051–1065, September 2022. doi:10.1162/tacl_a_00505.">12</a>,<a
                              class="reference internal" href="#id25"
                              title="Maureen de Seyssel, Jie Chi, Skyler Seto, Maartje ter Hoeve, Masha Fedzechkina, and Natalie Schluter. Discriminating Form and Meaning in Multilingual Models with Minimal-Pair ABX Tasks. June 2025. arXiv:2505.17747 [cs]. doi:10.48550/arXiv.2505.17747.">13</a></sup></span>
                        <br>
                        <a class="reference external" href="https://www.ru.nl/personen/bentum-m">Martijn Bentum</a>, <a
                          class="reference external"
                          href="https://www.illc.uva.nl/People/Table/person/5440/Charlotte-Pouw">Charlotte Pouw</a>
                      </p>
                    </div>
                  </blockquote>
                  <blockquote>
                    <div>
                      <p>Tutorial on <strong>Feature Importance Scoring methods</strong> for speech model
                        interpretability, including: <br> Context-mixing (Attention<span id="id7"><sup><a
                              class="reference internal" href="#id26"
                              title="Puyuan Peng and David Harwath. Word Discovery in Visually Grounded, Self-Supervised Speech Models. In Interspeech 2022, 2823–2827. 2022. doi:10.21437/Interspeech.2022-10652.">14</a>,<a
                              class="reference internal" href="#id27"
                              title="Erfan A Shams, Iona Gessinger, and Julie Carson-Berndsen. Uncovering syllable constituents in the self-attention-based speech representations of whisper. In Yonatan Belinkov, Najoung Kim, Jaap Jumelet, Hosein Mohebbi, Aaron Mueller, and Hanjie Chen, editors, Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, 238–247. Miami, Florida, US, November 2024. Association for Computational Linguistics. doi:10.18653/v1/2024.blackboxnlp-1.16.">15</a></sup></span>,
                        Value-Zeroing<span id="id8"><sup><a class="reference internal" href="#id28"
                              title="Hosein Mohebbi, Grzegorz Chrupała, Willem Zuidema, and Afra Alishahi. Homophone Disambiguation Reveals Patterns of Context Mixing in Speech Transformers. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 8249–8260. Singapore, December 2023. Association for Computational Linguistics. doi:10.18653/v1/2023.emnlp-main.513.">16</a></sup></span>),
                        <br> Feature attribution<span id="id9"><sup><a class="reference internal" href="#id29"
                              title="Dennis Fucci, Beatrice Savoldi, Marco Gaido, Matteo Negri, Mauro Cettolo, and Luisa Bentivogli. Explainability for Speech Models: On the Challenges of Acoustic Feature Selection. In Felice Dell'Orletta, Alessandro Lenci, Simonetta Montemagni, and Rachele Sprugnoli, editors, Proceedings of the 10th Italian Conference on Computational Linguistics (CLiC-it 2024), 373–381. Pisa, Italy, December 2024. CEUR Workshop Proceedings.">17</a>,<a
                              class="reference internal" href="#id30"
                              title="Gaofei Shen, Hosein Mohebbi, Arianna Bisazza, Afra Alishahi, and Grzegorz Chrupała. On the reliability of feature attribution methods for speech classification. In Interspeech 2025. 2025. doi:10.48550/arXiv.2505.16406.">18</a></sup></span>
                        (Gradient-based<span id="id10"><sup><a class="reference internal" href="#id31"
                              title="Archiki Prasad and Preethi Jyothi. How Accents Confound: Probing for Accent Information in End-to-End Speech Recognition Systems. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 3739–3753. Online, July 2020. Association for Computational Linguistics. URL: https://aclanthology.org/2020.acl-main.345 (visited on 2024-10-10), doi:10.18653/v1/2020.acl-main.345.">19</a>,<a
                              class="reference internal" href="#id32"
                              title="Shubham Gupta, Mirco Ravanelli, Pascal Germain, and Cem Subakan. Phoneme Discretized Saliency Maps for Explainable Detection of AI-Generated Voice. In Interspeech 2024, 3295–3299. 2024. doi:10.21437/Interspeech.2024-632.">20</a></sup></span>
                        and Perturbation-based<span id="id11"><sup><a class="reference internal" href="#id33"
                              title="Xiaoliang Wu, Peter Bell, and Ajitha Rajan. Explanations for automatic speech recognition. In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), volume, 1-5. 2023. doi:10.1109/ICASSP49357.2023.10094635.">21</a>,<a
                              class="reference internal" href="#id34"
                              title="Eliana Pastor, Alkis Koudounas, Giuseppe Attanasio, Dirk Hovy, and Elena Baralis. Explaining speech classification models via word-level audio segments and paralinguistic features. In Yvette Graham and Matthew Purver, editors, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), 2221–2238. St. Julian's, Malta, March 2024. Association for Computational Linguistics. URL: https://aclanthology.org/2024.eacl-long.136/.">22</a></sup></span>)
                        <br>
                        <a class="reference external" href="https://hmohebbi.github.io/">Hosein Mohebbi</a>, <a
                          class="reference external" href="https://www.gaofeishen.com/">Gaofei Shen</a>
                      </p>
                    </div>
                  </blockquote>
                  <blockquote>
                    <div>
                      <p><strong>Outlook</strong> on future work in interpretability, <strong>Discussion</strong> of key
                        takeaways and findings <br>
                        <a class="reference external" href="https://mdhk.net/">Marianne de Heer Kloots</a>, <a
                          class="reference external" href="https://www.tilburguniversity.edu/staff/t-o-lentz">Tom
                          Lentz</a>, <a class="reference external" href="https://staff.fnwi.uva.nl/w.zuidema/">Willem
                          Zuidema</a>
                      </p>
                    </div>
                  </blockquote>
                </section>
                <section id="references">
                  <h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
                  <div class="docutils container" id="id12">
                    <div class="citation" id="id13" role="doc-biblioentry">
                      <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span
                          class="fn-bracket">]</span></span>
                      <p>Martijn Bentum, Louis ten Bosch, and Tom Lentz. The Processing of Stress in End-to-End
                        Automatic Speech Recognition Models. In <em>Interspeech 2024</em>, 2350–2354. 2024. <a
                          class="reference external"
                          href="https://doi.org/10.21437/Interspeech.2024-44">doi:10.21437/Interspeech.2024-44</a>.</p>
                    </div>
                    <div class="citation" id="id14" role="doc-biblioentry">
                      <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">2</a><span
                          class="fn-bracket">]</span></span>
                      <p>Gaofei Shen, Michaela Watkins, Afra Alishahi, Arianna Bisazza, and Grzegorz Chrupała. Encoding
                        of lexical tone in self-supervised models of spoken language. In Kevin Duh, Helena Gomez, and
                        Steven Bethard, editors, <em>Proceedings of the 2024 Conference of the North American Chapter of
                          the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long
                          Papers)</em>, 4250–4261. Mexico City, Mexico, June 2024. Association for Computational
                        Linguistics. <a class="reference external"
                          href="https://doi.org/10.18653/v1/2024.naacl-long.239">doi:10.18653/v1/2024.naacl-long.239</a>.
                      </p>
                    </div>
                    <div class="citation" id="id15" role="doc-biblioentry">
                      <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">3</a><span
                          class="fn-bracket">]</span></span>
                      <p>Marianne de Heer Kloots, Hosein Mohebbi, Charlotte Pouw, Gaofei Shen, Willem Zuidema, and
                        Martijn Bentum. What do self-supervised speech models know about Dutch? Analyzing advantages of
                        language-specific pre-training. In <em>Interspeech 2025</em>. 2025. <a
                          class="reference external"
                          href="https://doi.org/10.48550/arXiv.2506.00981">doi:10.48550/arXiv.2506.00981</a>.</p>
                    </div>
                    <div class="citation" id="id16" role="doc-biblioentry">
                      <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">4</a><span
                          class="fn-bracket">]</span></span>
                      <p>Patrick Cormac English, John D. Kelleher, and Julie Carson-Berndsen. Domain-Informed Probing of
                        wav2vec 2.0 Embeddings for Phonetic Features. In Garrett Nicolai and Eleanor Chodroff, editors,
                        <em>Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonetics,
                          Phonology, and Morphology</em>, 83–91. Seattle, Washington, July 2022. Association for
                        Computational Linguistics. <a class="reference external"
                          href="https://doi.org/10.18653/v1/2022.sigmorphon-1.9">doi:10.18653/v1/2022.sigmorphon-1.9</a>.
                      </p>
                    </div>
                    <div class="citation" id="id18" role="doc-biblioentry">
                      <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">5</a><span
                          class="fn-bracket">]</span></span>
                      <p>Grzegorz Chrupała, Bertrand Higy, and Afra Alishahi. Analyzing analytical methods: the case of
                        phonology in neural models of spoken language. In Dan Jurafsky, Joyce Chai, Natalie Schluter,
                        and Joel Tetreault, editors, <em>Proceedings of the 58th Annual Meeting of the Association for
                          Computational Linguistics</em>, 4146–4156. Online, July 2020. Association for Computational
                        Linguistics. <a class="reference external"
                          href="https://doi.org/10.18653/v1/2020.acl-main.381">doi:10.18653/v1/2020.acl-main.381</a>.
                      </p>
                    </div>
                    <div class="citation" id="id17" role="doc-biblioentry">
                      <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">6</a><span
                          class="fn-bracket">]</span></span>
                      <p>Gaofei Shen, Afra Alishahi, Arianna Bisazza, and Grzegorz Chrupała. Wave to Syntax: Probing
                        spoken language models for syntax. In <em>Interspeech 2023</em>, 1259–1263. 2023. <a
                          class="reference external"
                          href="https://doi.org/10.21437/Interspeech.2023-679">doi:10.21437/Interspeech.2023-679</a>.
                      </p>
                    </div>
                    <div class="citation" id="id19" role="doc-biblioentry">
                      <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">7</a><span
                          class="fn-bracket">]</span></span>
                      <p>Ankita Pasad, Ju-Chieh Chou, and Karen Livescu. Layer-wise analysis of a self-supervised speech
                        representation model. In <em>2021 IEEE Automatic Speech Recognition and Understanding Workshop
                          (ASRU)</em>, volume, 914–921. 2021. <a class="reference external"
                          href="https://doi.org/10.1109/ASRU51503.2021.9688093">doi:10.1109/ASRU51503.2021.9688093</a>.
                      </p>
                    </div>
                    <div class="citation" id="id20" role="doc-biblioentry">
                      <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">8</a><span
                          class="fn-bracket">]</span></span>
                      <p>Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural
                        network representations revisited. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
                        <em>Proceedings of the 36th International Conference on Machine Learning</em>, volume 97 of
                        Proceedings of Machine Learning Research, 3519–3529. PMLR, 09–15 Jun 2019. URL: <a
                          class="reference external"
                          href="https://proceedings.mlr.press/v97/kornblith19a.html">https://proceedings.mlr.press/v97/kornblith19a.html</a>.
                      </p>
                    </div>
                    <div class="citation" id="id21" role="doc-biblioentry">
                      <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">9</a><span
                          class="fn-bracket">]</span></span>
                      <p>Marianne de Heer Kloots and Willem Zuidema. Human-like Linguistic Biases in Neural Speech
                        Models: Phonetic Categorization and Phonotactic Constraints in Wav2Vec2.0. In <em>Interspeech
                          2024</em>, 4593–4597. 2024. <a class="reference external"
                          href="https://doi.org/10.21437/Interspeech.2024-2490">doi:10.21437/Interspeech.2024-2490</a>.
                      </p>
                    </div>
                    <div class="citation" id="id22" role="doc-biblioentry">
                      <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink"
                          href="#id5">10</a><span class="fn-bracket">]</span></span>
                      <p>Anna Langedijk, Hosein Mohebbi, Gabriele Sarti, Willem Zuidema, and Jaap Jumelet. DecoderLens:
                        Layerwise Interpretation of Encoder-Decoder Transformers. In Kevin Duh, Helena Gomez, and Steven
                        Bethard, editors, <em>Findings of the Association for Computational Linguistics: NAACL
                          2024</em>, 4764–4780. Mexico City, Mexico, June 2024. Association for Computational
                        Linguistics. <a class="reference external"
                          href="https://doi.org/10.18653/v1/2024.findings-naacl.296">doi:10.18653/v1/2024.findings-naacl.296</a>.
                      </p>
                    </div>
                    <div class="citation" id="id23" role="doc-biblioentry">
                      <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink"
                          href="#id6">11</a><span class="fn-bracket">]</span></span>
                      <p>Thomas Schatz. <em>ABX-Discriminability Measures and Applications</em>. Theses, Université
                        Paris 6 (UPMC), September 2016. URL: <a class="reference external"
                          href="https://hal.science/tel-01407461">https://hal.science/tel-01407461</a>.</p>
                    </div>
                    <div class="citation" id="id24" role="doc-biblioentry">
                      <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink"
                          href="#id6">12</a><span class="fn-bracket">]</span></span>
                      <p>Robin Algayres, Tristan Ricoul, Julien Karadayi, Hugo Laurençon, Salah Zaiem, Abdelrahman
                        Mohamed, Benoît Sagot, and Emmanuel Dupoux. DP-Parse: Finding Word Boundaries from Raw Speech
                        with an Instance Lexicon. <em>Transactions of the Association for Computational
                          Linguistics</em>, 10:1051–1065, September 2022. <a class="reference external"
                          href="https://doi.org/10.1162/tacl_a_00505">doi:10.1162/tacl_a_00505</a>.</p>
                    </div>
                    <div class="citation" id="id25" role="doc-biblioentry">
                      <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink"
                          href="#id6">13</a><span class="fn-bracket">]</span></span>
                      <p>Maureen de Seyssel, Jie Chi, Skyler Seto, Maartje ter Hoeve, Masha Fedzechkina, and Natalie
                        Schluter. Discriminating Form and Meaning in Multilingual Models with Minimal-Pair ABX Tasks.
                        June 2025. arXiv:2505.17747 [cs]. <a class="reference external"
                          href="https://doi.org/10.48550/arXiv.2505.17747">doi:10.48550/arXiv.2505.17747</a>.</p>
                    </div>
                    <div class="citation" id="id26" role="doc-biblioentry">
                      <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink"
                          href="#id7">14</a><span class="fn-bracket">]</span></span>
                      <p>Puyuan Peng and David Harwath. Word Discovery in Visually Grounded, Self-Supervised Speech
                        Models. In <em>Interspeech 2022</em>, 2823–2827. 2022. <a class="reference external"
                          href="https://doi.org/10.21437/Interspeech.2022-10652">doi:10.21437/Interspeech.2022-10652</a>.
                      </p>
                    </div>
                    <div class="citation" id="id27" role="doc-biblioentry">
                      <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink"
                          href="#id7">15</a><span class="fn-bracket">]</span></span>
                      <p>Erfan A Shams, Iona Gessinger, and Julie Carson-Berndsen. Uncovering syllable constituents in
                        the self-attention-based speech representations of whisper. In Yonatan Belinkov, Najoung Kim,
                        Jaap Jumelet, Hosein Mohebbi, Aaron Mueller, and Hanjie Chen, editors, <em>Proceedings of the
                          7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP</em>, 238–247.
                        Miami, Florida, US, November 2024. Association for Computational Linguistics. <a
                          class="reference external"
                          href="https://doi.org/10.18653/v1/2024.blackboxnlp-1.16">doi:10.18653/v1/2024.blackboxnlp-1.16</a>.
                      </p>
                    </div>
                    <div class="citation" id="id28" role="doc-biblioentry">
                      <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink"
                          href="#id8">16</a><span class="fn-bracket">]</span></span>
                      <p>Hosein Mohebbi, Grzegorz Chrupała, Willem Zuidema, and Afra Alishahi. Homophone Disambiguation
                        Reveals Patterns of Context Mixing in Speech Transformers. In Houda Bouamor, Juan Pino, and
                        Kalika Bali, editors, <em>Proceedings of the 2023 Conference on Empirical Methods in Natural
                          Language Processing</em>, 8249–8260. Singapore, December 2023. Association for Computational
                        Linguistics. <a class="reference external"
                          href="https://doi.org/10.18653/v1/2023.emnlp-main.513">doi:10.18653/v1/2023.emnlp-main.513</a>.
                      </p>
                    </div>
                    <div class="citation" id="id29" role="doc-biblioentry">
                      <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink"
                          href="#id9">17</a><span class="fn-bracket">]</span></span>
                      <p>Dennis Fucci, Beatrice Savoldi, Marco Gaido, Matteo Negri, Mauro Cettolo, and Luisa Bentivogli.
                        Explainability for Speech Models: On the Challenges of Acoustic Feature Selection. In Felice
                        Dell'Orletta, Alessandro Lenci, Simonetta Montemagni, and Rachele Sprugnoli, editors,
                        <em>Proceedings of the 10th Italian Conference on Computational Linguistics (CLiC-it 2024)</em>,
                        373–381. Pisa, Italy, December 2024. CEUR Workshop Proceedings.
                      </p>
                    </div>
                    <div class="citation" id="id30" role="doc-biblioentry">
                      <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink"
                          href="#id9">18</a><span class="fn-bracket">]</span></span>
                      <p>Gaofei Shen, Hosein Mohebbi, Arianna Bisazza, Afra Alishahi, and Grzegorz Chrupała. On the
                        reliability of feature attribution methods for speech classification. In <em>Interspeech
                          2025</em>. 2025. <a class="reference external"
                          href="https://doi.org/10.48550/arXiv.2505.16406">doi:10.48550/arXiv.2505.16406</a>.</p>
                    </div>
                    <div class="citation" id="id31" role="doc-biblioentry">
                      <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink"
                          href="#id10">19</a><span class="fn-bracket">]</span></span>
                      <p>Archiki Prasad and Preethi Jyothi. How Accents Confound: Probing for Accent Information in
                        End-to-End Speech Recognition Systems. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel
                        Tetreault, editors, <em>Proceedings of the 58th Annual Meeting of the Association for
                          Computational Linguistics</em>, 3739–3753. Online, July 2020. Association for Computational
                        Linguistics. URL: <a class="reference external"
                          href="https://aclanthology.org/2020.acl-main.345">https://aclanthology.org/2020.acl-main.345</a>
                        (visited on 2024-10-10), <a class="reference external"
                          href="https://doi.org/10.18653/v1/2020.acl-main.345">doi:10.18653/v1/2020.acl-main.345</a>.
                      </p>
                    </div>
                    <div class="citation" id="id32" role="doc-biblioentry">
                      <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink"
                          href="#id10">20</a><span class="fn-bracket">]</span></span>
                      <p>Shubham Gupta, Mirco Ravanelli, Pascal Germain, and Cem Subakan. Phoneme Discretized Saliency
                        Maps for Explainable Detection of AI-Generated Voice. In <em>Interspeech 2024</em>, 3295–3299.
                        2024. <a class="reference external"
                          href="https://doi.org/10.21437/Interspeech.2024-632">doi:10.21437/Interspeech.2024-632</a>.
                      </p>
                    </div>
                    <div class="citation" id="id33" role="doc-biblioentry">
                      <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink"
                          href="#id11">21</a><span class="fn-bracket">]</span></span>
                      <p>Xiaoliang Wu, Peter Bell, and Ajitha Rajan. Explanations for automatic speech recognition. In
                        <em>ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing
                          (ICASSP)</em>, volume, 1–5. 2023. <a class="reference external"
                          href="https://doi.org/10.1109/ICASSP49357.2023.10094635">doi:10.1109/ICASSP49357.2023.10094635</a>.
                      </p>
                    </div>
                    <div class="citation" id="id34" role="doc-biblioentry">
                      <span class="label"><span class="fn-bracket">[</span><a role="doc-backlink"
                          href="#id11">22</a><span class="fn-bracket">]</span></span>
                      <p>Eliana Pastor, Alkis Koudounas, Giuseppe Attanasio, Dirk Hovy, and Elena Baralis. Explaining
                        speech classification models via word-level audio segments and paralinguistic features. In
                        Yvette Graham and Matthew Purver, editors, <em>Proceedings of the 18th Conference of the
                          European Chapter of the Association for Computational Linguistics (Volume 1: Long
                          Papers)</em>, 2221–2238. St. Julian's, Malta, March 2024. Association for Computational
                        Linguistics. URL: <a class="reference external"
                          href="https://aclanthology.org/2024.eacl-long.136/">https://aclanthology.org/2024.eacl-long.136/</a>.
                      </p>
                    </div>
                  </div>
          </div>
          <div>
            </section>
            <div class="toctree-wrapper compound">
            </div>
            </section>

            <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
            <script>kernelName = 'python3'</script>

            </article>






            <footer class="prev-next-footer">

              <div class="prev-next-area">
                <a class="right-next" href="interspeech2025/intro.html" title="next page">
                  <div class="prev-next-info">
                    <p class="prev-next-subtitle">next</p>
                    <p class="prev-next-title">TBA, check back soon!</p>
                  </div>
                  <i class="fa-solid fa-angle-right"></i>
                </a>
              </div>
            </footer>

          </div>



          <div class="bd-sidebar-secondary bd-toc">
            <div class="sidebar-secondary-items sidebar-secondary__inner">


              <div class="sidebar-secondary-item">
                <div class="page-toc tocsection onthispage">
                  <i class="fa-solid fa-list"></i> Contents
                </div>
                <nav class="bd-toc-nav page-toc">
                  <ul class="visible nav section-nav flex-column">
                    <li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link"
                        href="#preliminary-programme-at-interspeech-2025">Preliminary programme at Interspeech 2025</a>
                    </li>
                    <li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link"
                        href="#references">References</a></li>
                  </ul>
                </nav>
              </div>

            </div>
          </div>


        </div>
        <footer class="bd-footer-content">

          <div class="bd-footer-content__inner container">

            <div class="footer-item">

            </div>

            <div class="footer-item">


            </div>

            <div class="footer-item">

            </div>

            <div class="footer-item">

              <div class="extra_footer">
                <div>
                  <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><img
                      src="https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png"></a>
                </div>

              </div>
            </div>

          </div>
        </footer>


      </main>
    </div>
  </div>

  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
</body>

</html>